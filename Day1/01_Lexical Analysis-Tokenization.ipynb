{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"They told that their ages are 25 27 and 31 respectively\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average of ages mentioned in the above sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['25', '27', '31']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages = re.findall(r\"\\d+\",sent)\n",
    "\n",
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages = [int(age)for age in ages]\n",
    "len(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.66666666666667"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg = 0 \n",
    "\n",
    "for age in ages:\n",
    "    avg = avg+int(age)/len(ages)\n",
    "\n",
    "avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([int(word) for word in sent.split() if word.isdigit()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Hello friends ! How are you ? Welcome to Python Programming.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the functions \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends !', 'How are you ?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segmentation\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the percentage of puncutation symbols present in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '?', '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc = [word for word in lst if not word.isalnum()]\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [word for word in lst]\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage = (len(punc)/len(a))*100\n",
    "\n",
    "percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.getsizeof('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤®à¤¾à¤¨à¤¸ à¤•à¥à¤²à¤•à¤°à¥à¤£à¥€'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char =\"\\u092E\\u093E\\u0928\\u0938 \\u0915\\u0941\\u0932\\u0915\\u0930\\u094D\\u0923\\u0940\"\n",
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤®à¤¾à¤¨à¤¸', 'à¤•à¥à¤²à¤•à¤°à¥à¤£à¥€']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.find(\"à¤¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [ 'à¤¯à¥‹à¤—à¥‡à¤¶', 'à¤•à¥à¤£à¤¾à¤²', 'à¤®à¤¾à¤¨à¤¸à¥€', 'à¤®à¤¾à¤¨à¤¸', 'à¤†à¤¦à¤¿à¤¤à¥à¤¯', 'à¤ªà¤¾à¤°à¥à¤¥' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤®à¤¾à¤¨à¤¸à¥€\n",
      "à¤®à¤¾à¤¨à¤¸\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    if name.startswith('à¤®à¤¾'):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtext = 'à¥ªà¥® à¤•à¤¿. à¤®à¥€. à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤†à¤£à¤¿ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¥à¤¹à¥à¤¯à¤¾à¤¤à¥€à¤² à¤µà¥‡à¤²à¥à¤¹à¥‡ à¤¤à¤¾à¤²à¥à¤•à¥à¤¯à¤¾à¤¤ à¤µ à¤­à¥‹à¤° à¤—à¤¾à¤µà¤¾à¤šà¥à¤¯à¤¾ à¤µà¤¾à¤¯à¤µà¥à¤¯à¥‡à¤²à¤¾ à¥¨à¥ª à¤•à¤¿.à¤®à¥€. à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤¨à¥€à¤°à¤¾-à¤µà¥‡à¤³à¤µà¤‚à¤¡à¥€-à¤•à¤¾à¤¨à¤‚à¤¦à¥€ à¤†à¤£à¤¿ à¤—à¥à¤‚à¤œà¤µà¤£à¥€ à¤¯à¤¾ à¤¨à¤¦à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤–à¥‹à¤±à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤¬à¥‡à¤šà¤•à¥à¤¯à¤¾à¤¤ à¤®à¥à¤°à¥à¤‚à¤¬à¤¦à¥‡à¤µà¤¾à¤šà¤¾ à¤¡à¥‹à¤‚à¤—à¤° à¤‰à¤­à¤¾ à¤†à¤¹à¥‡. à¤®à¤¾à¤µà¤³ à¤­à¤¾à¤—à¤¾à¤®à¤§à¥à¤¯à¥‡ à¤°à¤¾à¤œà¥à¤¯à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¸à¤¾à¤§à¥à¤¯ à¤•à¤°à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€ à¤°à¤¾à¤œà¤—à¤¡ à¤†à¤£à¤¿ à¤¤à¥‹à¤°à¤£à¤¾ à¤¹à¥‡ à¤¦à¥‹à¤¨à¥à¤¹à¥€ à¤•à¤¿à¤²à¥à¤²à¥‡ à¤®à¥‹à¤•à¥à¤¯à¤¾à¤šà¥à¤¯à¤¾ à¤ à¤¿à¤•à¤¾à¤£à¥€ à¤¹à¥‹à¤¤à¥‡. à¤¤à¥‹à¤°à¤£à¤¾ Archived 2020-09-20 at the Wayback Machine. à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤•à¤¾à¤°à¤¾à¤¨à¥‡ à¤²à¤¹à¤¾à¤¨ à¤…à¤¸à¤²à¥à¤¯à¤¾à¤®à¥à¤³à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¹à¤¾ à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¸à¥‹à¤¯à¥€à¤šà¤¾ à¤¨à¤µà¥à¤¹à¤¤à¤¾. à¤¤à¥à¤¯à¤¾à¤®à¤¾à¤¨à¤¾à¤¨à¥‡ à¤°à¤¾à¤œà¤—à¤¡ à¤¦à¥à¤°à¥à¤—à¤® à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¬à¤°à¤¾à¤š à¤®à¥‹à¤ à¤¾ à¤†à¤¹à¥‡. à¤¶à¤¿à¤µà¤¾à¤¯ à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤•à¤¡à¥‡ à¤•à¥‹à¤£à¤¤à¥à¤¯à¤¾à¤¹à¥€ à¤¬à¤¾à¤œà¥‚à¤¨à¥‡ à¤¯à¥‡à¤¤à¤¾à¤¨à¤¾ à¤à¤–à¤¾à¤¦à¥€ à¤Ÿà¥‡à¤•à¤¡à¥€ à¤•à¤¿à¤‚à¤µà¤¾ à¤¨à¤¦à¥€ à¤“à¤²à¤¾à¤‚à¤¡à¤¾à¤µà¥€à¤š à¤²à¤¾à¤—à¤¤à¥‡. à¤à¤µà¤¢à¥€ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€,à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤†à¤ªà¤²à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤¨à¥€ Archived 2020-03-18 at the Wayback Machine. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥€ à¤¨à¤¿à¤µà¤¡ à¤•à¥‡à¤²à¥€. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤²à¤¾ à¤¤à¥€à¤¨ à¤®à¤¾à¤šà¥à¤¯à¤¾ à¤µ à¤à¤• à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤¹à¥‡. à¤°à¤¾à¤œà¤—à¤¡à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤–à¥‚à¤ª à¤‰à¤‚à¤š à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¥€ à¤¸à¤®à¥à¤¦à¥à¤°à¤¸à¤ªà¤¾à¤Ÿà¥€à¤ªà¤¾à¤¸à¥‚à¤¨à¤šà¥€ à¤‰à¤‚à¤šà¥€ à¥§à¥©à¥¯à¥ª à¤®à¥€à¤Ÿà¤° à¤†à¤¹à¥‡.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¥ªà¥® à¤•à¤¿. à¤®à¥€. à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤†à¤£à¤¿ à¤ªà¥à¤£à¥‡ à¤œà¤¿à¤²à¥à¤¹à¥à¤¯à¤¾à¤¤à¥€à¤² à¤µà¥‡à¤²à¥à¤¹à¥‡ à¤¤à¤¾à¤²à¥à¤•à¥à¤¯à¤¾à¤¤ à¤µ à¤­à¥‹à¤° à¤—à¤¾à¤µà¤¾à¤šà¥à¤¯à¤¾ à¤µà¤¾à¤¯à¤µà¥à¤¯à¥‡à¤²à¤¾ à¥¨à¥ª à¤•à¤¿.à¤®à¥€. à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤° à¤¨à¥€à¤°à¤¾-à¤µà¥‡à¤³à¤µà¤‚à¤¡à¥€-à¤•à¤¾à¤¨à¤‚à¤¦à¥€ à¤†à¤£à¤¿ à¤—à¥à¤‚à¤œà¤µà¤£à¥€ à¤¯à¤¾ à¤¨à¤¦à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤–à¥‹à¤±à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾ à¤¬à¥‡à¤šà¤•à¥à¤¯à¤¾à¤¤ à¤®à¥à¤°à¥à¤‚à¤¬à¤¦à¥‡à¤µà¤¾à¤šà¤¾ à¤¡à¥‹à¤‚à¤—à¤° à¤‰à¤­à¤¾ à¤†à¤¹à¥‡. à¤®à¤¾à¤µà¤³ à¤­à¤¾à¤—à¤¾à¤®à¤§à¥à¤¯à¥‡ à¤°à¤¾à¤œà¥à¤¯à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¸à¤¾à¤§à¥à¤¯ à¤•à¤°à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€ à¤°à¤¾à¤œà¤—à¤¡ à¤†à¤£à¤¿ à¤¤à¥‹à¤°à¤£à¤¾ à¤¹à¥‡ à¤¦à¥‹à¤¨à¥à¤¹à¥€ à¤•à¤¿à¤²à¥à¤²à¥‡ à¤®à¥‹à¤•à¥à¤¯à¤¾à¤šà¥à¤¯à¤¾ à¤ à¤¿à¤•à¤¾à¤£à¥€ à¤¹à¥‹à¤¤à¥‡. à¤¤à¥‹à¤°à¤£à¤¾ Archived 2020-09-20 at the Wayback Machine. à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤•à¤¾à¤°à¤¾à¤¨à¥‡ à¤²à¤¹à¤¾à¤¨ à¤…à¤¸à¤²à¥à¤¯à¤¾à¤®à¥à¤³à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¹à¤¾ à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¸à¥‹à¤¯à¥€à¤šà¤¾ à¤¨à¤µà¥à¤¹à¤¤à¤¾. à¤¤à¥à¤¯à¤¾à¤®à¤¾à¤¨à¤¾à¤¨à¥‡ à¤°à¤¾à¤œà¤—à¤¡ à¤¦à¥à¤°à¥à¤—à¤® à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤¬à¤°à¤¾à¤š à¤®à¥‹à¤ à¤¾ à¤†à¤¹à¥‡. à¤¶à¤¿à¤µà¤¾à¤¯ à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤•à¤¡à¥‡ à¤•à¥‹à¤£à¤¤à¥à¤¯à¤¾à¤¹à¥€ à¤¬à¤¾à¤œà¥‚à¤¨à¥‡ à¤¯à¥‡à¤¤à¤¾à¤¨à¤¾ à¤à¤–à¤¾à¤¦à¥€ à¤Ÿà¥‡à¤•à¤¡à¥€ à¤•à¤¿à¤‚à¤µà¤¾ à¤¨à¤¦à¥€ à¤“à¤²à¤¾à¤‚à¤¡à¤¾à¤µà¥€à¤š à¤²à¤¾à¤—à¤¤à¥‡. à¤à¤µà¤¢à¥€ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€,à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤†à¤ªà¤²à¥‡ à¤°à¤¾à¤œà¤•à¥€à¤¯ à¤•à¥‡à¤‚à¤¦à¥à¤° à¤®à¥à¤¹à¤£à¥‚à¤¨ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤¨à¥€ Archived 2020-03-18 at the Wayback Machine. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥€ à¤¨à¤¿à¤µà¤¡ à¤•à¥‡à¤²à¥€. à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤²à¤¾ à¤¤à¥€à¤¨ à¤®à¤¾à¤šà¥à¤¯à¤¾ à¤µ à¤à¤• à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤†à¤¹à¥‡. à¤°à¤¾à¤œà¤—à¤¡à¤šà¤¾ à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾ à¤–à¥‚à¤ª à¤‰à¤‚à¤š à¤…à¤¸à¥‚à¤¨ à¤¤à¥à¤¯à¤¾à¤šà¥€ à¤¸à¤®à¥à¤¦à¥à¤°à¤¸à¤ªà¤¾à¤Ÿà¥€à¤ªà¤¾à¤¸à¥‚à¤¨à¤šà¥€ à¤‰à¤‚à¤šà¥€ à¥§à¥©à¥¯à¥ª à¤®à¥€à¤Ÿà¤° à¤†à¤¹à¥‡.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¥ªà¥®',\n",
       " 'à¤•à¤¿',\n",
       " '.',\n",
       " 'à¤®à¥€',\n",
       " '.',\n",
       " 'à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤°',\n",
       " 'à¤†à¤£à¤¿',\n",
       " 'à¤ªà¥à¤£à¥‡',\n",
       " 'à¤œà¤¿à¤²à¥à¤¹à¥à¤¯à¤¾à¤¤à¥€à¤²',\n",
       " 'à¤µà¥‡à¤²à¥à¤¹à¥‡',\n",
       " 'à¤¤à¤¾à¤²à¥à¤•à¥à¤¯à¤¾à¤¤',\n",
       " 'à¤µ',\n",
       " 'à¤­à¥‹à¤°',\n",
       " 'à¤—à¤¾à¤µà¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤µà¤¾à¤¯à¤µà¥à¤¯à¥‡à¤²à¤¾',\n",
       " 'à¥¨à¥ª',\n",
       " 'à¤•à¤¿.à¤®à¥€',\n",
       " '.',\n",
       " 'à¤…à¤‚à¤¤à¤°à¤¾à¤µà¤°',\n",
       " 'à¤¨à¥€à¤°à¤¾-à¤µà¥‡à¤³à¤µà¤‚à¤¡à¥€-à¤•à¤¾à¤¨à¤‚à¤¦à¥€',\n",
       " 'à¤†à¤£à¤¿',\n",
       " 'à¤—à¥à¤‚à¤œà¤µà¤£à¥€',\n",
       " 'à¤¯à¤¾',\n",
       " 'à¤¨à¤¦à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾',\n",
       " 'à¤–à¥‹à¤±à¥à¤¯à¤¾à¤‚à¤šà¥à¤¯à¤¾',\n",
       " 'à¤¬à¥‡à¤šà¤•à¥à¤¯à¤¾à¤¤',\n",
       " 'à¤®à¥à¤°à¥à¤‚à¤¬à¤¦à¥‡à¤µà¤¾à¤šà¤¾',\n",
       " 'à¤¡à¥‹à¤‚à¤—à¤°',\n",
       " 'à¤‰à¤­à¤¾',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.',\n",
       " 'à¤®à¤¾à¤µà¤³',\n",
       " 'à¤­à¤¾à¤—à¤¾à¤®à¤§à¥à¤¯à¥‡',\n",
       " 'à¤°à¤¾à¤œà¥à¤¯à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤°',\n",
       " 'à¤¸à¤¾à¤§à¥à¤¯',\n",
       " 'à¤•à¤°à¤£à¥à¤¯à¤¾à¤¸à¤¾à¤ à¥€',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡',\n",
       " 'à¤†à¤£à¤¿',\n",
       " 'à¤¤à¥‹à¤°à¤£à¤¾',\n",
       " 'à¤¹à¥‡',\n",
       " 'à¤¦à¥‹à¤¨à¥à¤¹à¥€',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¥‡',\n",
       " 'à¤®à¥‹à¤•à¥à¤¯à¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤ à¤¿à¤•à¤¾à¤£à¥€',\n",
       " 'à¤¹à¥‹à¤¤à¥‡',\n",
       " '.',\n",
       " 'à¤¤à¥‹à¤°à¤£à¤¾',\n",
       " 'Archived',\n",
       " '2020-09-20',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Wayback',\n",
       " 'Machine',\n",
       " '.',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¥à¤¯à¤¾à¤šà¤¾',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤†à¤•à¤¾à¤°à¤¾à¤¨à¥‡',\n",
       " 'à¤²à¤¹à¤¾à¤¨',\n",
       " 'à¤…à¤¸à¤²à¥à¤¯à¤¾à¤®à¥à¤³à¥‡',\n",
       " 'à¤°à¤¾à¤œà¤•à¥€à¤¯',\n",
       " 'à¤•à¥‡à¤‚à¤¦à¥à¤°',\n",
       " 'à¤®à¥à¤¹à¤£à¥‚à¤¨',\n",
       " 'à¤¹à¤¾',\n",
       " 'à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤¸à¥‹à¤¯à¥€à¤šà¤¾',\n",
       " 'à¤¨à¤µà¥à¤¹à¤¤à¤¾',\n",
       " '.',\n",
       " 'à¤¤à¥à¤¯à¤¾à¤®à¤¾à¤¨à¤¾à¤¨à¥‡',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡',\n",
       " 'à¤¦à¥à¤°à¥à¤—à¤®',\n",
       " 'à¤…à¤¸à¥‚à¤¨',\n",
       " 'à¤¤à¥à¤¯à¤¾à¤šà¤¾',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤¬à¤°à¤¾à¤š',\n",
       " 'à¤®à¥‹à¤ à¤¾',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤¯',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤•à¤¡à¥‡',\n",
       " 'à¤•à¥‹à¤£à¤¤à¥à¤¯à¤¾à¤¹à¥€',\n",
       " 'à¤¬à¤¾à¤œà¥‚à¤¨à¥‡',\n",
       " 'à¤¯à¥‡à¤¤à¤¾à¤¨à¤¾',\n",
       " 'à¤à¤–à¤¾à¤¦à¥€',\n",
       " 'à¤Ÿà¥‡à¤•à¤¡à¥€',\n",
       " 'à¤•à¤¿à¤‚à¤µà¤¾',\n",
       " 'à¤¨à¤¦à¥€',\n",
       " 'à¤“à¤²à¤¾à¤‚à¤¡à¤¾à¤µà¥€à¤š',\n",
       " 'à¤²à¤¾à¤—à¤¤à¥‡',\n",
       " '.',\n",
       " 'à¤à¤µà¤¢à¥€',\n",
       " 'à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤à¤¤à¤¾',\n",
       " 'à¤¹à¥‹à¤¤à¥€',\n",
       " ',',\n",
       " 'à¤®à¥à¤¹à¤£à¥‚à¤¨',\n",
       " 'à¤†à¤ªà¤²à¥‡',\n",
       " 'à¤°à¤¾à¤œà¤•à¥€à¤¯',\n",
       " 'à¤•à¥‡à¤‚à¤¦à¥à¤°',\n",
       " 'à¤®à¥à¤¹à¤£à¥‚à¤¨',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤œà¥€',\n",
       " 'à¤®à¤¹à¤¾à¤°à¤¾à¤œà¤¾à¤‚à¤¨à¥€',\n",
       " 'Archived',\n",
       " '2020-03-18',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Wayback',\n",
       " 'Machine',\n",
       " '.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤šà¥€',\n",
       " 'à¤¨à¤¿à¤µà¤¡',\n",
       " 'à¤•à¥‡à¤²à¥€',\n",
       " '.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤¾à¤²à¤¾',\n",
       " 'à¤¤à¥€à¤¨',\n",
       " 'à¤®à¤¾à¤šà¥à¤¯à¤¾',\n",
       " 'à¤µ',\n",
       " 'à¤à¤•',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.',\n",
       " 'à¤°à¤¾à¤œà¤—à¤¡à¤šà¤¾',\n",
       " 'à¤¬à¤¾à¤²à¥‡à¤•à¤¿à¤²à¥à¤²à¤¾',\n",
       " 'à¤–à¥‚à¤ª',\n",
       " 'à¤‰à¤‚à¤š',\n",
       " 'à¤…à¤¸à¥‚à¤¨',\n",
       " 'à¤¤à¥à¤¯à¤¾à¤šà¥€',\n",
       " 'à¤¸à¤®à¥à¤¦à¥à¤°à¤¸à¤ªà¤¾à¤Ÿà¥€à¤ªà¤¾à¤¸à¥‚à¤¨à¤šà¥€',\n",
       " 'à¤‰à¤‚à¤šà¥€',\n",
       " 'à¥§à¥©à¥¯à¥ª',\n",
       " 'à¤®à¥€à¤Ÿà¤°',\n",
       " 'à¤†à¤¹à¥‡',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(mtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! How are you?\n",
      "Welcome to the world of Python Programming\n"
     ]
    }
   ],
   "source": [
    "text = open(r'C:\\Users\\Administrator.DAI-PC2\\Desktop\\NLP\\Day1\\test.txt','r')\n",
    "data = text.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "# Create a object\n",
    "\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tab Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!\tHow are you?\n",
      "Welcome to the world of \tPython Programming\n"
     ]
    }
   ],
   "source": [
    "text = open(r'C:\\Users\\Administrator.DAI-PC2\\Desktop\\NLP\\Day1\\test.txt','r')\n",
    "data = text.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends!',\n",
       " 'How are you?\\nWelcome to the world of ',\n",
       " 'Python Programming']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "\n",
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "# Create a object\n",
    "\n",
    "tk = TabTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends!\\tHow are you?',\n",
       " 'Welcome to the world of \\tPython Programming']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "# Create a object\n",
    "\n",
    "tk = LineTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whitespace Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# Create a object\n",
    "\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MWE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"The Van Rossum is Python creator, visting Pune this week. The devlopmenrt comunity is very eager to meet Van Rossum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Van Rossum is Python creator, visting Pune this week. The devlopmenrt comunity is very eager to meet Van Rossum.\n"
     ]
    }
   ],
   "source": [
    "print(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'devlopmenrt',\n",
       " 'comunity',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'creator',\n",
       " ',',\n",
       " 'visting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'devlopmenrt',\n",
       " 'comunity',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van Rossum',\n",
       " '.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# Create a object\n",
    "\n",
    "tk = MWETokenizer(separator=' ')\n",
    "\n",
    "# add Multi Word Expression\n",
    "\n",
    "tk.add_mwe(('Van','Rossum'))\n",
    "\n",
    "# tokenize the data\n",
    "\n",
    "tk.tokenize(word_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!ðŸ‘How are you?\n",
      "Welcome to the world of ðŸ‘ŒPython Programming\n"
     ]
    }
   ],
   "source": [
    "text = open(r'C:\\Users\\Administrator.DAI-PC2\\Desktop\\NLP\\Day1\\test.txt','r')\n",
    "data = text.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " 'ðŸ‘',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'ðŸ‘Œ',\n",
       " 'Python',\n",
       " 'Programming']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Create a object\n",
    "\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "# tokenize the data\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "with\n",
      "punctuation\n",
      ">\n",
      "Let's\n",
      "tokenize\n",
      "it\n",
      "Is\n",
      "it\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,:?\\s]+\",text)\n",
    "\n",
    "text = \"This is some text with punctuation > Let's tokenize it. Is it ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "print(\"Tokens: \")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('student3.tsv')\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roll\\tname\\tclass\\tmarks\\tage',\n",
       " '1\\tanil\\tTE\\t56.77\\t22',\n",
       " '2\\tamit\\tTE\\t59.77\\t21',\n",
       " '3\\taniket\\tBE\\t76.88\\t19',\n",
       " '4\\tajinkya\\tTE\\t69.66\\t20',\n",
       " '5\\tasha\\tTE\\t63.28\\t20',\n",
       " '6\\tayesha\\tBE\\t49.55\\t20',\n",
       " '7\\tamar\\tBE\\t65.34\\t19',\n",
       " '8\\tamita\\tBE\\t68.33\\t23',\n",
       " '9\\tamol\\tTE\\t56.75\\t20',\n",
       " '10\\tanmol\\tBE\\t78.66\\t21',\n",
       " '']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.split('\\n')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list =[]\n",
    "\n",
    "for data in data:\n",
    "    row = data.split('\\t')\n",
    "    data_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['roll', 'name', 'class', 'marks', 'age'],\n",
       " ['1', 'anil', 'TE', '56.77', '22'],\n",
       " ['2', 'amit', 'TE', '59.77', '21'],\n",
       " ['3', 'aniket', 'BE', '76.88', '19'],\n",
       " ['4', 'ajinkya', 'TE', '69.66', '20'],\n",
       " ['5', 'asha', 'TE', '63.28', '20'],\n",
       " ['6', 'ayesha', 'BE', '49.55', '20'],\n",
       " ['7', 'amar', 'BE', '65.34', '19'],\n",
       " ['8', 'amita', 'BE', '68.33', '23'],\n",
       " ['9', 'amol', 'TE', '56.75', '20'],\n",
       " ['10', 'anmol', 'BE', '78.66', '21'],\n",
       " ['']]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = data_list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'anil', 'TE', '56.77', '22'],\n",
       " ['2', 'amit', 'TE', '59.77', '21'],\n",
       " ['3', 'aniket', 'BE', '76.88', '19'],\n",
       " ['4', 'ajinkya', 'TE', '69.66', '20'],\n",
       " ['5', 'asha', 'TE', '63.28', '20'],\n",
       " ['6', 'ayesha', 'BE', '49.55', '20'],\n",
       " ['7', 'amar', 'BE', '65.34', '19'],\n",
       " ['8', 'amita', 'BE', '68.33', '23'],\n",
       " ['9', 'amol', 'TE', '56.75', '20'],\n",
       " ['10', 'anmol', 'BE', '78.66', '21']]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNN_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
